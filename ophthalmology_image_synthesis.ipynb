{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative AI for Ophthalmological Image Synthesis\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This project aims to develop a generative AI model for creating synthetic ophthalmological images based on the Brazilian Multilabel Ophthalmological Dataset (BRSET). My goal is to generate high quality, diverse images that could potentially be used for augmenting datasets, improving model training, and advancing research in ophthalmology.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "My approach is inspired by the paper \"Using generative AI to investigate medical imagery models and datasets\" (Lang et al., 2024). I will implement a multistep process:\n",
    "\n",
    "1. **Image Classification**: Train a deep learning classifier on the BRSET dataset to predict various ophthalmological conditions.\n",
    "\n",
    "2. **Generative Model**: Develop a StyleGAN2-based generative model, incorporating guidance from my trained classifier.\n",
    "\n",
    "3. **Attribute Discovery**: Use the trained generator to identify and visualize key attributes that influence the classifier's predictions.\n",
    "\n",
    "4. **Analysis and Interpretation**: Examine the generated images and attributes to gain insights into the model's understanding of ophthalmological features.\n",
    "\n",
    "## Project Goals\n",
    "\n",
    "- Create a high-performance classifier for ophthalmological conditions using the BRSET dataset.\n",
    "- Implement a StyleGAN2 based generator capable of producing realistic eye images.\n",
    "- Discover and visualize attributes that are important for classifying various eye conditions.\n",
    "- Generate synthetic images that could potentially be used to augment existing datasets.\n",
    "\n",
    "## Ethical Considerations\n",
    "\n",
    "While this project aims to advance medical imaging research, we must be mindful of the ethical implications of generating synthetic medical data. All generated images should be clearly labeled as synthetic and not used for diagnostic purposes without extensive validation.\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "This notebook will guide you through the implementation of each step in my methodology. Let's begin by setting up our environment and loading the BRSET dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive base directory\n",
    "BASE_DIR = '/content/drive/MyDrive/'\n",
    "\n",
    "# Local Base Directory\n",
    "# BASE_DIR = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "from numpy.random import normal\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Deep learning and image processing\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# TensorFlow and Keras modules\n",
    "from tensorflow.keras import layers, models, optimizers, losses, metrics, backend, applications\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.mixed_precision import global_policy\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.utils import custom_object_scope\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping, TensorBoard, CSVLogger\n",
    "\n",
    "\n",
    "# Scikit-learn for data splitting and evaluation metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Stats libraries for statistical analysis\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 12\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Ensure TensorFlow is using GPU\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "# Enable eager execution\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "# Display all outputs in a cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Print environment information\n",
    "print(\"Num GPUs Available:\", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Keras version:\", tf.keras.__version__)\n",
    "print(\"Eager execution enabled:\", tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the labels dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data labels \n",
    "\n",
    "# Local Labels path\n",
    "#label_path = '/Volumes/Extreme SSD/a-brazilian-multilabel-ophthalmological-dataset-brset-1.0.0/labels.csv'\n",
    "\n",
    "# Google Drive Labels path \n",
    "label_path = '/content/drive/MyDrive/a-brazilian-multilabel-ophthalmological-dataset-brset-1.0.0/labels.csv'\n",
    "\n",
    "labels = pd.read_csv(label_path)\n",
    "\n",
    "# Display the first few rows of the data labels\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in images and inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the fundus photos\n",
    "\n",
    "# Local image path\n",
    "#IMAGE_PATH = '/Volumes/Extreme SSD/a-brazilian-multilabel-ophthalmological-dataset-brset-1.0.0/fundus_photos/'\n",
    "\n",
    "# Google drive image path \n",
    "IMAGE_PATH = '/content/drive/MyDrive/a-brazilian-multilabel-ophthalmological-dataset-brset-1.0.0/fundus_photos/'\n",
    "\n",
    "def load_and_preprocess_image(image_id, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Load and preprocess a fundus photo given its image_id.\n",
    "    \n",
    "    Args:\n",
    "    image_id (str): The ID of the image to load.\n",
    "    target_size (tuple): The target size to resize the image to.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.array: The preprocessed image as a numpy array.\n",
    "    \"\"\"\n",
    "    # Construct the full path to the image\n",
    "    image_path = os.path.join(IMAGE_PATH, f\"{image_id}.jpg\")\n",
    "    \n",
    "    # Load the image\n",
    "    img = load_img(image_path, target_size=target_size)\n",
    "    \n",
    "    # Convert the image to a numpy array\n",
    "    img_array = img_to_array(img)\n",
    "    \n",
    "    # Normalize the image\n",
    "    img_array = img_array / 255.0\n",
    "    \n",
    "    return img_array\n",
    "\n",
    "def load_batch_of_images(image_ids, batch_size=32):\n",
    "    \"\"\"\n",
    "    Load and preprocess a batch of fundus photos.\n",
    "    \n",
    "    Args:\n",
    "    image_ids (list): List of image IDs to load.\n",
    "    batch_size (int): Number of images to load at once.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.array: A batch of preprocessed images.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    for i in range(0, len(image_ids), batch_size):\n",
    "        batch_ids = image_ids[i:i+batch_size]\n",
    "        batch_images = [load_and_preprocess_image(id) for id in batch_ids]\n",
    "        images.extend(batch_images)\n",
    "    return np.array(images)\n",
    "\n",
    "# Load the first 100 images \n",
    "first_100_image_ids = labels['image_id'].iloc[:100].tolist()\n",
    "batch_of_images = load_batch_of_images(first_100_image_ids)\n",
    "\n",
    "print(f\"Batch of images shape: {batch_of_images.shape}\")\n",
    "\n",
    "# Display a grid of the first 16 images\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i+1);\n",
    "    plt.imshow(batch_of_images[i]);\n",
    "    plt.axis('off');\n",
    "    plt.title(f\"Image ID: {first_100_image_ids[i]}\");\n",
    "plt.tight_layout();\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA and dataset preparation/cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels data EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of binary diagnosis columns\n",
    "binary_diagnoses = ['diabetic_retinopathy', 'macular_edema', 'scar', 'nevus', 'amd',\n",
    "                    'vascular_occlusion', 'hypertensive_retinopathy', 'drusens',\n",
    "                    'hemorrhage', 'retinal_detachment', 'myopic_fundus', 'increased_cup_disc']\n",
    "\n",
    "# Calculate the counts and percentages in total patient population\n",
    "diagnosis_counts = labels[binary_diagnoses].sum().sort_values(ascending=False)\n",
    "total_patients = len(labels)\n",
    "diagnosis_percentages = (diagnosis_counts / total_patients) * 100\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "ax = diagnosis_counts.plot(kind='bar')\n",
    "\n",
    "# Add count and percentage labels on top of each bar\n",
    "for i, (count, percentage) in enumerate(zip(diagnosis_counts, diagnosis_percentages)):\n",
    "    ax.text(i, count, f'N={count}\\n({percentage:.1f}%)', \n",
    "            ha='center', va='bottom')\n",
    "\n",
    "plt.title('Distribution of Diagnoses')\n",
    "plt.xlabel('Diagnosis (N = count), percentage of total patients (%)')\n",
    "plt.ylabel('Count')\n",
    "plt.yticks(range(0, 3001, 500))\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest diagnosis frequency in the dataset is increased cup disc ratio (CDR), followed by drusens, and diabetic retinopathy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix of numerical columns and binary diagnoses\n",
    "numerical_columns = ['patient_age', 'patient_sex', 'exam_eye', 'DR_SDRG', 'DR_ICDR', \n",
    "                     'focus', 'iluminaton', 'image_field', 'artifacts']\n",
    "correlation_matrix = labels[numerical_columns + binary_diagnoses].corr()\n",
    "plt.figure(figsize=(20, 16))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out top correlations in the correlation matrix\n",
    "def print_top_correlations(correlation_matrix, n=20):\n",
    "    # Unstack the correlation matrix\n",
    "    correlations = correlation_matrix.unstack()\n",
    "\n",
    "    # Sort correlations in descending order of absolute value\n",
    "    correlations = correlations.abs().sort_values(ascending=False)\n",
    "\n",
    "    # Remove self correlations\n",
    "    correlations = correlations[correlations != 1.0]\n",
    "\n",
    "    # Create a set to keep track of pairs \n",
    "    seen_pairs = set()\n",
    "\n",
    "    print(f\"Top {n} Correlation Pairs:\")\n",
    "    count = 0\n",
    "    for (var1, var2), correlation in correlations.items():\n",
    "        pair = frozenset([var1, var2])\n",
    "\n",
    "        if pair not in seen_pairs:\n",
    "            print(f\"{var1} - {var2}: {correlation_matrix.loc[var1, var2]:.4f}\")\n",
    "            seen_pairs.add(pair)\n",
    "            count += 1\n",
    "\n",
    "            if count == n:\n",
    "                break\n",
    "\n",
    "correlation_matrix = labels[numerical_columns + binary_diagnoses].corr()\n",
    "print_top_correlations(correlation_matrix, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation analysis of the Brazilian Retinal Image Dataset (BRSET) reveals several interesting relationships between various ophthalmological parameters and diagnoses.\n",
    "\n",
    "1. **DR_SDRG - DR_ICDR (0.9853)**: \n",
    "   This extremely high correlation is expected as both are classification systems for diabetic retinopathy (DR). The Scottish Diabetic Retinopathy Grading Scheme (SDRG) and the International Clinical Diabetic Retinopathy (ICDR) scale are closely aligned in their assessment of DR severity.\n",
    "\n",
    "2. **DR_ICDR - diabetic_retinopathy (0.9173)** and **diabetic_retinopathy - DR_SDRG (0.9103)**:\n",
    "   These strong correlations indicate that both grading systems (ICDR and SDRG) are highly predictive of the presence of diabetic retinopathy. This validates the consistency between the binary classification (presence/absence) and the more detailed grading scales.\n",
    "\n",
    "3. **diabetic_retinopathy - macular_edema (0.5611)**:\n",
    "   This moderate positive correlation suggests that patients with diabetic retinopathy are more likely to also have macular edema. This is clinically significant as macular edema is a common complication of diabetic retinopathy.\n",
    "\n",
    "4. **macular_edema - DR_SDRG (0.5406)** and **macular_edema - DR_ICDR (0.5337)**:\n",
    "   These correlations further support the relationship between the severity of diabetic retinopathy (as measured by both scales) and the presence of macular edema. As the severity of DR increases, the likelihood of macular edema also increases.\n",
    "\n",
    "5. **patient_age - drusens (0.2179)**:\n",
    "   This weak positive correlation suggests that the presence of drusens (small yellow or white accumulations of extracellular material in the retina) is more common in older patients. This aligns with clinical knowledge, as drusens are often associated with age-related macular degeneration (AMD).\n",
    "\n",
    "6. **vascular_occlusion - hemorrhage (0.1816)**:\n",
    "   This weak positive correlation indicates a relationship between vascular occlusions and hemorrhages in the retina. This makes clinical sense, as occlusions can lead to bleeding in the affected blood vessels.\n",
    "\n",
    "7. **patient_age - amd (0.1284)**:\n",
    "   The weak positive correlation between age and age-related macular degeneration (AMD) is expected, as AMD is more prevalent in older populations.\n",
    "\n",
    "8. **drusens - DR_SDRG (-0.0976)**:\n",
    "   This very weak negative correlation might suggest a slight inverse relationship between the presence of drusens and the severity of diabetic retinopathy. However, given the low correlation coefficient, this relationship is likely not clinically significant and would require further investigation to determine if it's meaningful.\n",
    "\n",
    "These correlations provide valuable insights into the relationships between various ophthalmological conditions and patient characteristics in the BRSET dataset. They highlight the interconnected nature of diabetic retinopathy, macular edema, and age-related eye conditions. These findings can inform feature selection for machine learning models and guide further clinical research into the progression and comorbidities of retinal diseases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of camera types\n",
    "plt.figure(figsize=(10, 6))\n",
    "labels['camera'].value_counts().plot(kind='bar')\n",
    "plt.title('Distribution of Camera Types')\n",
    "plt.xlabel('Camera')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of image quality\n",
    "plt.figure(figsize=(10, 6))\n",
    "labels['quality'].value_counts().plot(kind='bar')\n",
    "plt.title('Distribution of Image Quality')\n",
    "plt.xlabel('Quality')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the plot above we can see there is a subset of images labeled as inadequate. We will remove these images from the dataset as they are not useful for training our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape before dropping inadequate images\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the inadequate quality images\n",
    "labels = labels[labels['quality'] != 'Inadequate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape after dropping inadequate quality images\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of DR severity (DR_ICDR)\n",
    "plt.figure(figsize=(10, 6))\n",
    "labels['DR_ICDR'].value_counts().sort_index().plot(kind='bar')\n",
    "plt.title('Distribution of Diabetic Retinopathy Severity (ICDR scale)')\n",
    "plt.xlabel('Severity')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=labels, x='patient_age', kde=True)\n",
    "plt.title('Distribution of Patient Age')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diabetes duration distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "labels['diabetes_time_y'] = pd.to_numeric(labels['diabetes_time_y'], errors='coerce')\n",
    "sns.histplot(data=labels, x='diabetes_time_y', kde=True)\n",
    "plt.title('Distribution of Diabetes Duration')\n",
    "plt.xlabel('Years')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship between age and diabetes duration\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=labels, x='patient_age', y='diabetes_time_y')\n",
    "plt.title('Relationship between Patient Age and Diabetes Duration')\n",
    "plt.xlabel('Patient Age')\n",
    "plt.ylabel('Diabetes Duration (years)')\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of patient sex\n",
    "plt.figure(figsize=(8, 6))\n",
    "labels['patient_sex'].map({1: 'Male', 2: 'Female'}).value_counts().plot(kind='bar')\n",
    "plt.title('Distribution of Patient Sex')\n",
    "plt.xlabel('Sex')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of examined eye\n",
    "plt.figure(figsize=(8, 6))\n",
    "labels['exam_eye'].map({1: 'Right', 2: 'Left'}).value_counts().plot(kind='bar')\n",
    "plt.title('Distribution of Examined Eye')\n",
    "plt.xlabel('Eye')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary Statistics:\")\n",
    "labels[numerical_columns + binary_diagnoses].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMissing Values:\")\n",
    "print(labels.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above there are a lot of missing values in the diabetes_time_y and the insulin columns. Also comorbidity column has a lot of missing values that should be inspected. Patient age has quite a few missing values but we will impute them with the median. \n",
    "\n",
    "The diabetes_time_y and insulin columns have a lot of missing values so we will drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the missing ages with the median age\n",
    "median_age = labels['patient_age'].median()\n",
    "labels['patient_age'] = labels['patient_age'].fillna(median_age)\n",
    "\n",
    "# Drop the diabetes_time_y column and insulin column \n",
    "labels = labels.drop(columns=['diabetes_time_y', 'insuline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels['comorbidities'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the comobrbidiy column, it looks like the large values are not meaningful so we will drop the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the comorbidities column\n",
    "labels = labels.drop(columns=['comorbidities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean, std, min, and max pixel values across initial 100 images\n",
    "mean_pixel_value = np.mean(batch_of_images)\n",
    "std_pixel_value = np.std(batch_of_images)\n",
    "min_pixel_value = np.min(batch_of_images)\n",
    "max_pixel_value = np.max(batch_of_images)\n",
    "\n",
    "print(f\"Mean pixel value: {mean_pixel_value:.4f}\")\n",
    "print(f\"Std dev of pixel values: {std_pixel_value:.4f}\")\n",
    "print(f\"Min pixel value: {min_pixel_value:.4f}\")\n",
    "print(f\"Max pixel value: {max_pixel_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of pixel intensities\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(batch_of_images.ravel(), bins=50, range=(0, 1))\n",
    "plt.title(\"Histogram of Pixel Intensities\")\n",
    "plt.xlabel(\"Pixel Intensity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate color channels\n",
    "red_channel = batch_of_images[:, :, :, 0]\n",
    "green_channel = batch_of_images[:, :, :, 1]\n",
    "blue_channel = batch_of_images[:, :, :, 2]\n",
    "\n",
    "# Plot histograms for each channel\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.hist(red_channel.ravel(), bins=50, color='red', alpha=0.7)\n",
    "plt.title(\"Red Channel\")\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.hist(green_channel.ravel(), bins=50, color='green', alpha=0.7)\n",
    "plt.title(\"Green Channel\")\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.hist(blue_channel.ravel(), bins=50, color='blue', alpha=0.7)\n",
    "plt.title(\"Blue Channel\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image channel correlations\n",
    "r_g_corr = pearsonr(red_channel.ravel(), green_channel.ravel())[0]\n",
    "r_b_corr = pearsonr(red_channel.ravel(), blue_channel.ravel())[0]\n",
    "g_b_corr = pearsonr(green_channel.ravel(), blue_channel.ravel())[0]\n",
    "\n",
    "print(f\"Correlation between Red and Green channels: {r_g_corr:.4f}\")\n",
    "print(f\"Correlation between Red and Blue channels: {r_b_corr:.4f}\")\n",
    "print(f\"Correlation between Green and Blue channels: {g_b_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate brightness \n",
    "brightness = np.mean(batch_of_images, axis=3)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(brightness.ravel(), bins=50)\n",
    "plt.title(\"Histogram of Image Brightness\")\n",
    "plt.xlabel(\"Brightness\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate contrast\n",
    "contrast = np.std(batch_of_images, axis=3)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(contrast.ravel(), bins=50)\n",
    "plt.title(\"Histogram of Image Contrast\")\n",
    "plt.xlabel(\"Contrast\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building \n",
    "\n",
    "The model will focus on predicting diabetic retinopathy. Diabetic retinopathy is a common complication of diabetes and a leading cause of blindness in adults. Early detection and treatment are crucial for preventing vision loss. The model will be trained on the BRSET dataset, which contains retinal fundus images labeled with various ophthalmological conditions, including diabetic retinopathy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFIER = 'diabetic_retinopathy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[CLASSIFIER].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will sample randomly the same number of images as there are positive cases of diabetic retinopathy. This will help us balance the dataset. We will only sample images that have a diagnosis of diabetic retinopathy and images that have no other diagnosis for the other conditions in this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all rows where drusens == 1\n",
    "classifier_positive = labels[labels[CLASSIFIER] == 1]\n",
    "\n",
    "# Get the count of positive cases\n",
    "condition_positive = len(classifier_positive)\n",
    "\n",
    "# Create a mask for normal images (all 0 in the binary diagnosis columns)\n",
    "normal_mask = (\n",
    "    (labels['diabetic_retinopathy'] == 0) &\n",
    "    (labels['macular_edema'] == 0) &\n",
    "    (labels['scar'] == 0) &\n",
    "    (labels['nevus'] == 0) &\n",
    "    (labels['amd'] == 0) &\n",
    "    (labels['vascular_occlusion'] == 0) &\n",
    "    (labels['hypertensive_retinopathy'] == 0) &\n",
    "    (labels['drusens'] == 0) &\n",
    "    (labels['hemorrhage'] == 0) &\n",
    "    (labels['retinal_detachment'] == 0) &\n",
    "    (labels['myopic_fundus'] == 0) &\n",
    "    (labels['increased_cup_disc'] == 0) &\n",
    "    (labels['other'] == 0)\n",
    ")\n",
    "\n",
    "# Get all normal images\n",
    "normal_images = labels[normal_mask]\n",
    "\n",
    "# Randomly sample the same number of normal images as drusens positive cases\n",
    "sampled_normal = normal_images.sample(n=condition_positive, random_state=12)\n",
    "\n",
    "# Combine drusens positive cases and sampled normal images\n",
    "subset_df = pd.concat([classifier_positive, sampled_normal])\n",
    "\n",
    "# Shuffle the combined dataframe\n",
    "subset_df = subset_df.sample(frac=1, random_state=12).reset_index(drop=True)\n",
    "\n",
    "print(f\"Total number of normal (no diagnosis) images {len(normal_images)}\")\n",
    "print(f\"Sampled normal: {len(sampled_normal)}\")\n",
    "print(f\"{CLASSIFIER} positive: {len(classifier_positive)}\")\n",
    "print(f\"Total samples in combined subset: {len(subset_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df = subset_df.drop(columns = ['camera', 'nationality', 'other', 'quality', 'patient_id'], errors = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df[CLASSIFIER].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df[CLASSIFIER].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_diagnoses = [CLASSIFIER]\n",
    "\n",
    "IMAGE_SIZE = (224, 224)\n",
    "\n",
    "def parse_image(filename, labels):\n",
    "    image = tf.io.read_file(filename)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, IMAGE_SIZE)  # Resize to 224x224\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Normalize 0-1\n",
    "    return image, labels\n",
    "\n",
    "def augment_image(image, labels):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
    "    return image, labels\n",
    "\n",
    "def create_dataset(labels_df, batch_size=32, shuffle=True, augment=False):\n",
    "    \n",
    "    filenames = labels_df['image_id'].apply(lambda x: os.path.join(IMAGE_PATH, x + '.jpg')).tolist()\n",
    "    \n",
    "    # Get labels\n",
    "    labels = labels_df[binary_diagnoses].values.astype(np.float32).tolist()\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    \n",
    "    # Parse images and labels\n",
    "    dataset = dataset.map(parse_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    if augment:\n",
    "        dataset = dataset.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=1000)\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(subset_df, test_size=0.2, random_state=12)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = create_dataset(train_df, batch_size=BATCH_SIZE, shuffle=True, augment=True)\n",
    "val_dataset = create_dataset(val_df, batch_size=BATCH_SIZE, shuffle=False, augment=False)\n",
    "\n",
    "# Inspect the number of batches in the training and validation datasets\n",
    "print(f\"\\nNumber of batches in training dataset: {tf.data.experimental.cardinality(train_dataset)}\")\n",
    "print(f\"Number of batches in validation dataset: {tf.data.experimental.cardinality(val_dataset)}\")\n",
    "\n",
    "# Inspect the first batch of the training dataset\n",
    "for images, labels_batch in train_dataset.take(1):\n",
    "    print(f\"\\nShape of the image batch: {images.shape}\")\n",
    "    print(f\"Shape of the labels batch: {labels_batch.shape}\")\n",
    "    print(f\"Sample labels from the first image: {labels_batch[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the first batch of the training dataset\n",
    "for images, labels_batch in train_dataset.take(1):\n",
    "    print(f\"\\nShape of the image batch: {images.shape}\")\n",
    "    print(f\"Shape of the labels batch: {labels_batch.shape}\")\n",
    "    print(\"\\nLabels for each image in the batch:\")\n",
    "    for i, labels in enumerate(labels_batch):\n",
    "        print(f\"Image {i+1}: {labels.numpy()}\")\n",
    "\n",
    "# Print unique label combinations\n",
    "print(\"\\nUnique label combinations in the batch:\")\n",
    "unique_labels = np.unique(labels_batch.numpy(), axis=0)\n",
    "for label_combo in unique_labels:\n",
    "    print(label_combo)\n",
    "\n",
    "# Count of each label\n",
    "print(\"\\nCount of each label in the batch:\")\n",
    "label_counts = np.sum(labels_batch.numpy(), axis=0)\n",
    "for i, count in enumerate(label_counts):\n",
    "    print(f\"{binary_diagnoses[i]}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image classifier\n",
    "\n",
    "First we will start with a simple classifier, then the second classifier will be a vgg16 model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = models.Sequential([\n",
    "    layers.Conv2D(32, 3, activation='relu', input_shape=(224, 224, 3)),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(128, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(128, 3, activation='relu'),\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Optimizer for model \n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "classification_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Define callbacks\n",
    "\n",
    "# Learning rate scheduler\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0000001)\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Combine all callbacks\n",
    "callbacks = [\n",
    "    reduce_lr,\n",
    "    \n",
    "]\n",
    "\n",
    "# Train the model with all callbacks\n",
    "history = classification_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=150,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "classification_model.save(f'{BASE_DIR}classification_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VGG16 Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vgg16 model with sigmoid activation for binary classification\n",
    "def create_vgg16(input_shape=(224, 224, 3), num_classes=1):\n",
    "    model = models.Sequential([\n",
    "        # Block 1\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2), strides=(2, 2)),\n",
    "        \n",
    "        # Block 2\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2), strides=(2, 2)),\n",
    "        \n",
    "        # Block 3\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2), strides=(2, 2)),\n",
    "        \n",
    "        # Block 4\n",
    "        layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "        layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "        layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2), strides=(2, 2)),\n",
    "        \n",
    "        # Block 5\n",
    "        layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "        layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "        layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2), strides=(2, 2)),\n",
    "        \n",
    "        # Classification block\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(4096, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(4096, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='sigmoid' if num_classes == 1 else 'softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "vgg_model = create_vgg16()\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "vgg_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0000001)\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [reduce_lr]\n",
    "\n",
    "# Train the model\n",
    "history = vgg_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=150,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Update saved weights\n",
    "vgg_model.save(f'{BASE_DIR}vgg_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StylexGenerator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StylexGenerator(tf.keras.Model):\n",
    "    def __init__(self, latent_dim, img_shape, **kwargs):\n",
    "        super(StylexGenerator, self).__init__(**kwargs)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.img_shape = img_shape\n",
    "        self.model = models.Sequential([\n",
    "            layers.Dense(256, input_dim=latent_dim),\n",
    "            layers.LeakyReLU(alpha=0.2),\n",
    "            layers.BatchNormalization(momentum=0.8),\n",
    "            layers.Dense(512),\n",
    "            layers.LeakyReLU(alpha=0.2),\n",
    "            layers.BatchNormalization(momentum=0.8),\n",
    "            layers.Dense(1024),\n",
    "            layers.LeakyReLU(alpha=0.2),\n",
    "            layers.BatchNormalization(momentum=0.8),\n",
    "            layers.Dense(int(tf.math.reduce_prod(img_shape)), activation='tanh'), \n",
    "            layers.Reshape(img_shape)\n",
    "        ])\n",
    "\n",
    "    def call(self, z):\n",
    "        return self.model(z)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(StylexGenerator, self).get_config()\n",
    "        config.update({\n",
    "            \"latent_dim\": self.latent_dim,\n",
    "            \"img_shape\": self.img_shape\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "class StylexDiscriminator(tf.keras.Model):\n",
    "    def __init__(self, img_shape, **kwargs):\n",
    "        super(StylexDiscriminator, self).__init__(**kwargs)\n",
    "        self.img_shape = img_shape\n",
    "        self.model = models.Sequential([\n",
    "            layers.Flatten(input_shape=img_shape),\n",
    "            layers.Dense(512),\n",
    "            layers.LeakyReLU(alpha=0.2),\n",
    "            layers.Dense(256),\n",
    "            layers.LeakyReLU(alpha=0.2),\n",
    "            layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "    def call(self, img):\n",
    "        return self.model(img)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(StylexDiscriminator, self).get_config()\n",
    "        config.update({\n",
    "            \"img_shape\": self.img_shape\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "# Define loss functions\n",
    "def generator_loss(fake_output):\n",
    "    return tf.keras.losses.binary_crossentropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = tf.keras.losses.binary_crossentropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = tf.keras.losses.binary_crossentropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def classifier_loss(labels, cls_outputs):\n",
    "    return tf.keras.losses.binary_crossentropy(labels, cls_outputs)\n",
    "\n",
    "# Define the train_step function\n",
    "@tf.function\n",
    "def train_step(real_images, labels, generator, discriminator, classifier, gen_optimizer, disc_optimizer, threshold=0.5):\n",
    "    batch_size = tf.shape(real_images)[0]\n",
    "    noise = tf.random.normal([batch_size, latent_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(real_images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = tf.reduce_mean(generator_loss(fake_output))\n",
    "        disc_loss = tf.reduce_mean(discriminator_loss(real_output, fake_output))\n",
    "\n",
    "        # Classifier guidance loss\n",
    "        cls_outputs = classifier(generated_images, training=False)\n",
    "        cls_outputs_binary = tf.cast(cls_outputs > threshold, tf.float32)\n",
    "        c_loss = tf.reduce_mean(classifier_loss(labels, cls_outputs_binary))\n",
    "\n",
    "        gen_total_loss = gen_loss + c_loss\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_total_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    gen_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    disc_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss, c_loss\n",
    "\n",
    "# Setup model training\n",
    "img_shape = (224, 224, 3)  \n",
    "latent_dim = 100\n",
    "\n",
    "generator = StylexGenerator(latent_dim, img_shape)\n",
    "discriminator = StylexDiscriminator(img_shape)\n",
    "\n",
    "gen_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "disc_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "\n",
    "user_model = tf.keras.models.load_model(f'{BASE_DIR}classification_model.keras')\n",
    "dataset = train_dataset\n",
    "\n",
    "def generate_and_save_images(generator, epoch, num_examples=16):\n",
    "    # Generate noise for the input\n",
    "    noise = tf.random.normal([num_examples, generator.latent_dim])\n",
    "    \n",
    "    # Generate images\n",
    "    generated_images = generator(noise, training=False)\n",
    "\n",
    "    # Rescale images to [0, 1] \n",
    "    generated_images = (generated_images + 1) / 2.0 if generated_images.numpy().min() < 0 else generated_images\n",
    "    \n",
    "    # Plot the generated images\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "    for i in range(num_examples):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(generated_images[i])\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{BASE_DIR}generated_images_epoch_{epoch}.png')\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Images saved for epoch {epoch}\")\n",
    "\n",
    "epochs = 200\n",
    "checkpoint_interval = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    gen_losses = []\n",
    "    disc_losses = []\n",
    "    cls_losses = []\n",
    "\n",
    "    \n",
    "    for image_batch, label_batch in tqdm(dataset, desc=f'Epoch {epoch + 1}/{epochs}', unit='batch'):\n",
    "        gen_loss, disc_loss, c_loss = train_step(image_batch, label_batch, generator, discriminator, user_model, gen_optimizer, disc_optimizer)\n",
    "        gen_losses.append(gen_loss.numpy())\n",
    "        disc_losses.append(disc_loss.numpy())\n",
    "        cls_losses.append(c_loss.numpy())\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}, Gen Loss: {np.mean(gen_losses):.4f}, Disc Loss: {np.mean(disc_losses):.4f}, Classifier Loss: {np.mean(cls_losses):.4f}')\n",
    "\n",
    "    # Save checkpoint models\n",
    "    if (epoch + 1) % checkpoint_interval == 0:\n",
    "        generator.save(f'{BASE_DIR}stylex_generator_epoch_{epoch+1}.keras')\n",
    "        discriminator.save(f'{BASE_DIR}stylex_discriminator_epoch_{epoch+1}.keras')\n",
    "\n",
    "    # Generate and save sample images\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        generate_and_save_images(generator, epoch + 1)\n",
    "\n",
    "# Save the final models\n",
    "generator.save(f'{BASE_DIR}stylex_generator.keras')\n",
    "discriminator.save(f'{BASE_DIR}stylex_discriminator.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stylegan2_train, stylegan2_val = train_test_split(labels, test_size=0.2, random_state=12)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Create datasets\n",
    "stylegan2_train_dataset = create_dataset(stylegan2_train, batch_size=BATCH_SIZE, shuffle=True, augment=True)\n",
    "stylegan2_val_dataset = create_dataset(stylegan2_val, batch_size=BATCH_SIZE, shuffle=False, augment=False)\n",
    "\n",
    "# Inspect the number of batches in the training and validation datasets\n",
    "print(f\"\\nNumber of batches in training dataset: {tf.data.experimental.cardinality(stylegan2_train_dataset)}\")\n",
    "print(f\"Number of batches in validation dataset: {tf.data.experimental.cardinality(stylegan2_val_dataset)}\")\n",
    "\n",
    "# Inspect the first batch of the training dataset\n",
    "for images, labels_batch in stylegan2_train_dataset.take(1):\n",
    "    print(f\"\\nShape of the image batch: {images.shape}\")\n",
    "    print(f\"Shape of the labels batch: {labels_batch.shape}\")\n",
    "    print(f\"Sample labels from the first image: {labels_batch[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train the StyleGAN2 model on entre BRSET dataset, not just the diabetic retinopathy images. This will aid in generating synthetic images that are representative of the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StyleGAN2 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # StyleGAN2 Generator and Discriminator\n",
    "class AdaIN(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AdaIN, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        content_shape, style_shape = input_shape\n",
    "        self.channels = content_shape[-1]\n",
    "        self.style_scale = self.add_weight(name=\"style_scale\", shape=(style_shape[-1], self.channels), initializer=\"random_normal\")\n",
    "        self.style_bias = self.add_weight(name=\"style_bias\", shape=(style_shape[-1], self.channels), initializer=\"random_normal\")\n",
    "        super(AdaIN, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        content, style = inputs\n",
    "        mean, var = tf.nn.moments(content, axes=[1, 2], keepdims=True)\n",
    "        normalized = (content - mean) / tf.sqrt(var + 1e-8)\n",
    "        \n",
    "        style = tf.expand_dims(style, axis=1)\n",
    "        style = tf.expand_dims(style, axis=1)\n",
    "        \n",
    "        scale = tf.matmul(style, self.style_scale)\n",
    "        bias = tf.matmul(style, self.style_bias)\n",
    "        \n",
    "        return scale * normalized + bias\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "class StyleBlock(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, **kwargs):\n",
    "        super(StyleBlock, self).__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = layers.Conv2D(filters, kernel_size, padding=\"same\", use_bias=False)\n",
    "        self.adain = AdaIN()\n",
    "        self.activation = layers.LeakyReLU(0.2)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, w = inputs\n",
    "        x = self.conv(x)\n",
    "        x = self.adain([x, w])\n",
    "        return self.activation(x)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"filters\": self.filters,\n",
    "            \"kernel_size\": self.kernel_size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "class MappingNetwork(keras.Model):\n",
    "    def __init__(self, latent_dim, n_layers=8, **kwargs):\n",
    "        super(MappingNetwork, self).__init__(**kwargs)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.layers_list = []\n",
    "        for _ in range(n_layers):\n",
    "            self.layers_list.append(layers.Dense(latent_dim, activation='leaky_relu'))\n",
    "        self.layers_list.append(layers.Dense(latent_dim))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.layers_list:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"latent_dim\": self.latent_dim,\n",
    "            \"n_layers\": self.n_layers\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "class StyleGAN2Generator(keras.Model):\n",
    "    def __init__(self, latent_dim, **kwargs):\n",
    "        super(StyleGAN2Generator, self).__init__(**kwargs)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.mapping = MappingNetwork(latent_dim)\n",
    "        \n",
    "        self.input_dense = layers.Dense(7 * 7 * 512)\n",
    "        \n",
    "        self.conv_blocks = [\n",
    "            StyleBlock(512, 3),\n",
    "            StyleBlock(256, 3),\n",
    "            StyleBlock(128, 3),\n",
    "            StyleBlock(64, 3),\n",
    "            StyleBlock(32, 3),\n",
    "        ]\n",
    "        \n",
    "        self.to_rgb = layers.Conv2D(3, 1, padding=\"same\", activation=\"tanh\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        w = self.mapping(inputs)\n",
    "        \n",
    "        x = self.input_dense(w)\n",
    "        x = layers.Reshape((7, 7, 512))(x)\n",
    "        \n",
    "        for block in self.conv_blocks:\n",
    "            x = block([x, w])\n",
    "            x = layers.UpSampling2D()(x)\n",
    "        \n",
    "        return self.to_rgb(x)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"latent_dim\": self.latent_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "class StyleGAN2Discriminator(keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(StyleGAN2Discriminator, self).__init__(**kwargs)\n",
    "        self.conv_blocks = [\n",
    "            layers.Conv2D(64, 3, strides=2, padding=\"same\"),\n",
    "            layers.Conv2D(128, 3, strides=2, padding=\"same\"),\n",
    "            layers.Conv2D(256, 3, strides=2, padding=\"same\"),\n",
    "            layers.Conv2D(512, 3, strides=2, padding=\"same\"),\n",
    "            layers.Conv2D(512, 3, strides=2, padding=\"same\"),\n",
    "        ]\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense1 = layers.Dense(512, activation='leaky_relu')\n",
    "        self.dense2 = layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if isinstance(inputs, tuple):\n",
    "            x = inputs[0]  # Take only the image, ignore the label\n",
    "        else:\n",
    "            x = inputs\n",
    "        \n",
    "        for block in self.conv_blocks:\n",
    "            x = block(x)\n",
    "            x = layers.LeakyReLU(0.2)(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        return self.dense2(x)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "# Loss functions\n",
    "def generator_loss(fake_output):\n",
    "    return tf.keras.losses.binary_crossentropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = tf.keras.losses.binary_crossentropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = tf.keras.losses.binary_crossentropy(tf.zeros_like(fake_output), fake_output)\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "# Training step\n",
    "@tf.function\n",
    "def train_step(real_images, generator, discriminator, gen_optimizer, disc_optimizer, batch_size, latent_dim):\n",
    "    if isinstance(real_images, tuple):\n",
    "        real_images = real_images[0]  # Take only the images, ignore the labels\n",
    "    \n",
    "    noise = tf.random.normal([batch_size, latent_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(real_images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    gen_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    disc_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return tf.reduce_mean(gen_loss), tf.reduce_mean(disc_loss)\n",
    "\n",
    "# Setup and training\n",
    "latent_dim = 100\n",
    "batch_size = 16\n",
    "img_shape = (224, 224, 3)\n",
    "checkpoint_interval = 50\n",
    "\n",
    "generator = StyleGAN2Generator(latent_dim)\n",
    "discriminator = StyleGAN2Discriminator()\n",
    "\n",
    "# Recompile the models\n",
    "gen_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.0, beta_2=0.99, epsilon=1e-8)\n",
    "disc_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.0, beta_2=0.99, epsilon=1e-8)\n",
    "\n",
    "\n",
    "epochs = 500\n",
    "for epoch in range(epochs):\n",
    "    total_gen_loss = 0.0\n",
    "    total_disc_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    progress_bar = tqdm(stylegan2_train, desc=f'Epoch {epoch + 1}/{epochs}')\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        gen_loss, disc_loss = train_step(batch, generator, discriminator, gen_optimizer, disc_optimizer, batch_size, latent_dim)\n",
    "        total_gen_loss += gen_loss.numpy()\n",
    "        total_disc_loss += disc_loss.numpy()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Update progress bar description with current losses\n",
    "        progress_bar.set_postfix({\n",
    "            'Gen Loss': f'{gen_loss.numpy():.4f}',\n",
    "            'Disc Loss': f'{disc_loss.numpy():.4f}'\n",
    "        })\n",
    "    \n",
    "    avg_gen_loss = total_gen_loss / num_batches\n",
    "    avg_disc_loss = total_disc_loss / num_batches\n",
    "    \n",
    "    print(f'\\nEpoch {epoch + 1}, Avg Gen Loss: {avg_gen_loss:.4f}, Avg Disc Loss: {avg_disc_loss:.4f}')\n",
    "\n",
    "    # Save checkpoint models\n",
    "    if (epoch + 1) % checkpoint_interval == 0:\n",
    "        generator.save(f'{BASE_DIR}stylegan2_generator_epoch_{epoch+1}.keras')\n",
    "        discriminator.save(f'{BASE_DIR}stylegan2_discriminator_epoch_{epoch+1}.keras')\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        # Generate and save sample images\n",
    "        noise = tf.random.normal([1, latent_dim])\n",
    "        generated_images = generator(noise, training=False)\n",
    "        # Save the generated image\n",
    "        plt.imshow(generated_images[0] * 0.5 + 0.5)  # Rescale from [-1, 1] to [0, 1]\n",
    "        plt.axis('off')\n",
    "        plt.savefig(f'{BASE_DIR}generated_image_epoch_{epoch+1}.png')\n",
    "        plt.close()\n",
    "        \n",
    "\n",
    "generator.save(f'{BASE_DIR}stylegan2_generator.keras')\n",
    "discriminator.save(f'{BASE_DIR}stylegan2_discriminator.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attributes(generator, classifier, num_samples, latent_dim, num_attributes, batch_size=100):\n",
    "    attributes = []\n",
    "    for i in range(latent_dim):\n",
    "        noise = tf.random.normal([num_samples, latent_dim])\n",
    "        base_imgs = generator(noise)\n",
    "        base_preds = classifier(base_imgs)\n",
    "\n",
    "        pred_diffs = []\n",
    "        for start in range(0, num_samples, batch_size):\n",
    "            end = min(start + batch_size, num_samples)\n",
    "            noise_mod = noise[start:end].numpy()\n",
    "            noise_mod[:, i] += 0.1  # Small perturbation\n",
    "            mod_imgs = generator(noise_mod)\n",
    "            mod_preds = classifier(mod_imgs)\n",
    "\n",
    "            pred_diff = tf.reduce_mean(tf.abs(mod_preds - base_preds[start:end]))\n",
    "            pred_diffs.append(pred_diff.numpy())\n",
    "\n",
    "        avg_pred_diff = np.mean(pred_diffs)\n",
    "        attributes.append((i, avg_pred_diff))\n",
    "    \n",
    "    attributes.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [attr[0] for attr in attributes[:num_attributes]]\n",
    "\n",
    "def visualize_attribute(generator, attribute_idx, latent_dim):\n",
    "    noise = tf.random.normal([1, latent_dim])\n",
    "    base_img = generator(noise)\n",
    "    \n",
    "    noise_mod = noise.numpy()\n",
    "    noise_mod[0, attribute_idx] += 1  # Increase attribute\n",
    "    mod_img = generator(noise_mod)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(base_img[0].numpy())\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mod_img[0].numpy())\n",
    "    plt.title(f\"Modified Image (Attribute {attribute_idx})\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StyleGAN2 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models\n",
    "custom_objects = {\n",
    "    'StyleGAN2Generator': StyleGAN2Generator,\n",
    "    'StyleGAN2Discriminator': StyleGAN2Discriminator,\n",
    "    'AdaIN': AdaIN,\n",
    "    'StyleBlock': StyleBlock,\n",
    "    'MappingNetwork': MappingNetwork\n",
    "}\n",
    "\n",
    "with keras.utils.custom_object_scope(custom_objects):\n",
    "    loaded_generator = keras.models.load_model(f'{BASE_DIR}stylegan2_generator.keras')\n",
    "    loaded_discriminator = keras.models.load_model(f'{BASE_DIR}stylegan2_discriminator.keras')\n",
    "\n",
    "print(\"Models loaded successfully!\")\n",
    "\n",
    "\n",
    "latent_dim = generator.latent_dim \n",
    "num_samples = 1000\n",
    "num_attributes = 10\n",
    "classifier = keras.models.load_model('{BASE_DIR}vgg_model.keras')\n",
    "top_attributes = extract_attributes(generator, classifier, num_samples, latent_dim, num_attributes)\n",
    "\n",
    "for attr in top_attributes:\n",
    "    visualize_attribute(generator, attr, latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stylex features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract from the Stylex model and show the top attributes\n",
    "# Load the model using custom_object_scope\n",
    "with custom_object_scope({'StylexGenerator': StylexGenerator, 'StylexDiscriminator': StylexDiscriminator}):\n",
    "    generator = tf.keras.models.load_model(f'{BASE_DIR}stylex_generator.keras')\n",
    "classifier = tf.keras.models.load_model(f'{BASE_DIR}classification_model.keras')\n",
    "\n",
    "latent_dim = 100\n",
    "num_samples = 1000\n",
    "num_attributes = 10\n",
    "\n",
    "top_attributes = extract_attributes(generator, classifier, num_samples, latent_dim, num_attributes)\n",
    "\n",
    "for attr in top_attributes:\n",
    "    visualize_attribute(generator, attr, latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_tensor_image",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
